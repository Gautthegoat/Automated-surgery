# ACT: Action Chunking with Transformers for Robotic Manipulation

This project implements the ACT (Action Chunking with Transformers) model for fine-grained bimanual robotic manipulation, based on the paper "Learning Fine-Grained Bimanual Manipulation with Low-Cost Hardware" by Tony Z. Zhao et al.

## Project Overview

This implementation focuses on learning robotic manipulation tasks from demonstration data. It uses a transformer-based architecture to predict sequences of robot actions based on visual input, current joint positions, and a learned style variable.

## Model Architecture

The model consists of several key components:
- ResNet18 for image feature extraction
- A transformer encoder for processing combined image and state information
- A transformer decoder for generating action sequences
- A style encoder for capturing demonstration variations

# Expected folder for training

data_folder/
    Frames/
        video.mp4
    Logs/
        logs.db3
    Extracted_data/
        extracted_logs.csv
        frame_1.png
        ...

## Remarks

In ACT paper, they used frozen batchNorm2D, keeping the spatial interpretation static, but they commented on the code that they should verify if good idea...
Also, they have the option on not taking only features from the final output but also intermediate layers.

In diffusion policy, they used a standard ResNet-18 (without pretraining) as the encoder with the following modifications: 1) Replace the global average pooling with a spatial softmax pooling to maintain spatial information Mandlekar et al. (2021). 2) Replace BatchNorm with GroupNorm Wu and He (2018) for stable training.